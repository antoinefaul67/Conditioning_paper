{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9664729-8b6a-4c6c-8c48-0b751e75aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\antoi\\Downloads\\GMCM-main\\GMCM-main\\myenv2\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\antoi\\Downloads\\GMCM-main\\GMCM-main\\myenv2\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\antoi\\Downloads\\GMCM-main\\GMCM-main\\myenv2\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copulogram as cp\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd=tfp.distributions\n",
    "tfb=tfp.bijectors\n",
    "from scipy import io\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import mixture\n",
    "import joblib as jbl\n",
    "import sys\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from mixture_models import GMCM, GMC\n",
    "import utils as utl\n",
    "from scipy.stats import multivariate_normal\n",
    "import rpy2\n",
    "import scoringrules as sr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09370cfb-c2c1-437d-bf52-4390988119cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(3110251)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67e2b5d-187e-4d44-b5c4-4ff4a34191be",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "\n",
    "# Read the CSV file from the URL\n",
    "df = pd.read_csv(url, header=None)\n",
    "\n",
    "# Select the columns\n",
    "df = df.iloc[:,[14,26,28,29]]\n",
    "df.columns = [\"14\",\"26\",\"28\",\"29\"]\n",
    "data = df.astype('float32').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4580d78e-5011-4b7b-ad1c-c9c2160bb657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_missingness(df, missing_percentage=0.1):\n",
    "    # Copy the DataFrame to avoid modifying the original\n",
    "    df_missing = df.copy()\n",
    "    # Calculate the number of missing values to introduce\n",
    "    n_missing = int(missing_percentage * df.size)\n",
    "    # Flatten the DataFrame indices\n",
    "    flat_indices = [(row, col) for row in range(df.shape[0]) for col in range(df.shape[1])]\n",
    "    # Select random indices for missing values\n",
    "    missing_indices = np.random.choice(range(len(flat_indices)), n_missing, replace=False)\n",
    "    \n",
    "    for idx in missing_indices:\n",
    "        i, j = flat_indices[idx]\n",
    "        df_missing[i, j] = np.nan\n",
    "        \n",
    "    return df_missing\n",
    "# df = your original DataFrame\n",
    "df_missing = introduce_missingness(data, missing_percentage=0.1)\n",
    "mask = ~np.all(np.isnan(df_missing), axis=1)\n",
    "\n",
    "# Filter the array to remove rows that are entirely NaN\n",
    "df_missing = df_missing[mask]\n",
    "train_idx = np.where(~np.isnan(df_missing).any(axis=1))[0]\n",
    "# Find rows with AT LEAST one missing value\n",
    "test_idx = np.where(np.isnan(df_missing).any(axis=1))[0]\n",
    "train_data = df_missing[train_idx,]         # train: no missing values\n",
    "test_data_with_missing = df_missing[test_idx,]\n",
    "test_data = data[test_idx,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6655c323-ecc1-42e7-a5aa-4353a2dceaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 376, Number of dimensions = 4\n",
      "Learning Marginals\n",
      "Marginals learnt in 6.87 s.\n",
      "@ Iter:0,                         Training error: -12.0,                         Validation error: nan,                         Time Elapsed: 6.8 s\n",
      "@ Iter:1000,                         Training error: -12.6,                         Validation error: nan,                         Time Elapsed: 14.3 s\n",
      "@ Iter:2000,                         Training error: -12.7,                         Validation error: nan,                         Time Elapsed: 21.5 s\n",
      "@ Iter:3000,                         Training error: -13.2,                         Validation error: nan,                         Time Elapsed: 28.4 s\n",
      "@ Iter:4000,                         Training error: -12.1,                         Validation error: nan,                         Time Elapsed: 35.6 s\n",
      "@ Iter:5000,                         Training error: -12.1,                         Validation error: nan,                         Time Elapsed: 43.2 s\n",
      "@ Iter:6000,                         Training error: -11.9,                         Validation error: nan,                         Time Elapsed: 50.4 s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_comps=2\n",
    "nsamps,ndims = train_data.shape\n",
    "print(f'Number of samples = {nsamps}, Number of dimensions = {ndims}')\n",
    "gmcm = GMCM(ndims)\n",
    "gmcm.fit_dist_IFM(train_data,\n",
    "                  n_comps=n_comps,\n",
    "                  method=\"GMCM\",\n",
    "                 data_vld= None,# fixed number of components\n",
    "                 batch_size=10,\n",
    "                 max_iters=6001,\n",
    "                 regularize=True,\n",
    "                 init = 'gmm', #(init option: 'gmm' warm starts gmcm learning compared to 'random')\n",
    "                 print_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e7740f-9b13-473f-a311-ce0aa4a17d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_indexes = list(range(ndims)) ## depends on the problem\n",
    "#list_indexes = [0,1]\n",
    "logits,mus,covs,_ = utl.vec2gmm_params(gmcm.ndims,gmcm.ncomps,gmcm.gmc.params)\n",
    "alphas = tf.math.softmax(logits)\n",
    "#dim_remove = list(set(list(range(self.ndims)))-set(dim_list))\n",
    "mus_new = tf.gather(mus,list_indexes, axis=1).numpy()\n",
    "covs_new = tf.TensorArray(tf.float32,gmcm.ncomps)\n",
    "for k in range(gmcm.ncomps):\n",
    "    temp_mat = covs[k].numpy()\n",
    "    covs_new = covs_new.write(k,temp_mat[np.ix_(list_indexes, list_indexes)])\n",
    "covs_new = covs_new.stack().numpy()\n",
    "marginal_covs = np.array([[matrix[j, j] for j in range(ndims)] for matrix in covs_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bde00117-a9d9-4fbc-bac9-61280ea15667",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "means = []\n",
    "covariances = []\n",
    "\n",
    "# Iterate over the 5 elements in gmcm.marg_dists\n",
    "for i in range(ndims):\n",
    "    marginal = gmcm.marg_dists[i][\"marginal\"]\n",
    "    weights.append(marginal.weights_)\n",
    "    means.append(marginal.means_.flatten())  # Flatten means to get a 1D array\n",
    "    covariances.append(marginal.covariances_.flatten())  # Flatten covariances to get a 1D array\n",
    "max_length = max(len(arr) for arr in weights)\n",
    "\n",
    "# Pad arrays with zeros to ensure consistent length\n",
    "padded_arrays = [np.pad(arr, (0, max_length - len(arr)), mode='constant') for arr in  weights]\n",
    "\n",
    "# Concatenate into a single NumPy array\n",
    "weights = np.vstack(padded_arrays)\n",
    "\n",
    "max_length = max(len(arr) for arr in means)\n",
    "\n",
    "# Pad arrays with zeros to ensure consistent length\n",
    "padded_arrays = [np.pad(arr, (0, max_length - len(arr)), mode='constant') for arr in  means]\n",
    "\n",
    "# Concatenate into a single NumPy array\n",
    "means = np.vstack(padded_arrays)\n",
    "\n",
    "max_length = max(len(arr) for arr in covariances)\n",
    "\n",
    "# Pad arrays with zeros to ensure consistent length\n",
    "padded_arrays = [np.pad(arr, (0, max_length - len(arr)), mode='constant') for arr in  covariances]\n",
    "\n",
    "# Concatenate into a single NumPy array\n",
    "covariances = np.vstack(padded_arrays)\n",
    "os.chdir('C:\\\\Users\\\\antoi\\\\Documents\\\\UNIBE\\\\Conditioning\\\\Paper conditioning\\\\experiments\\\\imputation')\n",
    "np.savetxt(\"weights_matrix_output.csv\", np.transpose(weights), delimiter=\",\")\n",
    "np.savetxt(\"means_matrix_output.csv\", np.transpose(means), delimiter=\",\")\n",
    "np.savetxt(\"sds_matrix_output.csv\", np.transpose(covariances), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09be5e0-7920-4bb6-8f2b-a3c3b7f315d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tst_uniform = gmcm.marg_bijector.inverse(test_data).numpy()\n",
    "data_tst_latent = gmcm.gmc.distribution.bijector.inverse(data_tst_uniform).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5376207a-36ff-4069-bee3-ee8d9dc670b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "\n",
    "def precompute_matrices(mus_new, covs_new, dimension_joint, n_comps):\n",
    "    precomputed_matrices = {}\n",
    "\n",
    "    for j in range(n_comps):\n",
    "        cov_matrix = covs_new[j]\n",
    "        \n",
    "        all_indices = np.arange(dimension_joint)\n",
    "        \n",
    "        for mask in range(1 << dimension_joint):  # Create patterns for missing/observed indices\n",
    "            indexes_missing = [i for i in range(dimension_joint) if mask & (1 << i)]\n",
    "            indexes_observed = list(set(all_indices) - set(indexes_missing))\n",
    "            \n",
    "            if len(indexes_missing) == 0:\n",
    "                continue\n",
    "            \n",
    "            key = (j, tuple(indexes_missing), tuple(indexes_observed))\n",
    "            \n",
    "            mean_vector = mus_new[j]\n",
    "            \n",
    "            mean_mis = mean_vector[indexes_missing]\n",
    "            mean_obs = mean_vector[indexes_observed]\n",
    "            \n",
    "            cov_mm = cov_matrix[np.ix_(indexes_missing, indexes_missing)]\n",
    "            cov_mo = cov_matrix[np.ix_(indexes_missing, indexes_observed)]\n",
    "            cov_oo = cov_matrix[np.ix_(indexes_observed, indexes_observed)]\n",
    "            \n",
    "            inv_cov_oo = np.linalg.inv(cov_oo)\n",
    "            cond_cov = cov_mm - cov_mo @ inv_cov_oo @ cov_mo.T\n",
    "            \n",
    "            precomputed_matrices[key] = {'mean_mis': mean_mis, 'mean_obs': mean_obs, \n",
    "                                         'cov_mo': cov_mo, 'inv_cov_oo': inv_cov_oo, 'cov_oo':cov_oo,\n",
    "                                         'cond_cov': cond_cov}\n",
    "    \n",
    "    return precomputed_matrices\n",
    "\n",
    "def compute_conditional_samples(precomputed_matrices, data_tst_latent, test_data_with_missing, mus_new, alphas, n_comps, n_samples, size_test):\n",
    "    list_results = []\n",
    "    conditional_parameters = []\n",
    "    for i in range(size_test):\n",
    "        dimension_predict = np.isnan(test_data_with_missing[i,]).sum()\n",
    "        \n",
    "        if dimension_predict != 0:\n",
    "            indexes_missing = np.where(np.isnan(test_data_with_missing[i,]))[0]\n",
    "            all_indices = np.arange(len(test_data_with_missing[i,]))\n",
    "            indexes_observed = list(set(all_indices) - set(indexes_missing))\n",
    "            conditioning_value = data_tst_latent[i, indexes_observed]\n",
    "            new_alphas = np.zeros(n_comps)\n",
    "\n",
    "            for j in range(n_comps):\n",
    "                key = (j, tuple(indexes_missing), tuple(indexes_observed))\n",
    "                \n",
    "                mean_obs = precomputed_matrices[key]['mean_obs']\n",
    "                cov_obs = precomputed_matrices[key]['cov_oo']\n",
    "                \n",
    "                marginal = multivariate_normal.pdf(conditioning_value, mean=mean_obs, cov=cov_obs)\n",
    "                \n",
    "                joint = 0\n",
    "                for k in range(n_comps):\n",
    "                    key_k = (k, tuple(indexes_missing), tuple(indexes_observed))\n",
    "                    mean_obs_k = precomputed_matrices[key_k]['mean_obs']\n",
    "                    cov_obs_k = precomputed_matrices[key_k]['cov_oo']\n",
    "                    \n",
    "                    joint += alphas[k] * multivariate_normal.pdf(conditioning_value, mean=mean_obs_k, cov=cov_obs_k)\n",
    "                \n",
    "                if joint != 0:\n",
    "                    new_alphas[j] = alphas[j] * (marginal / joint)\n",
    "            \n",
    "            new_alphas /= new_alphas.sum()\n",
    "            \n",
    "            risk_vector_gmcm = np.zeros((n_samples, dimension_predict))\n",
    "            \n",
    "            for n in range(n_samples):\n",
    "                new_component = np.random.choice(n_comps, p=new_alphas)\n",
    "                \n",
    "                key = (new_component, tuple(indexes_missing), tuple(indexes_observed))\n",
    "                params = precomputed_matrices[key]\n",
    "                \n",
    "                cond_mean = params['mean_mis'] + params['cov_mo'] @ params['inv_cov_oo'] @ (conditioning_value - params['mean_obs'])\n",
    "                cond_cov = params['cond_cov']\n",
    "                \n",
    "                if cond_mean.size == 1:\n",
    "                    result = np.random.normal(loc=cond_mean, scale=np.sqrt(cond_cov))\n",
    "                else:\n",
    "                    result = np.random.multivariate_normal(mean=cond_mean, cov=cond_cov)\n",
    "                \n",
    "                risk_vector_gmcm[n, :] = result\n",
    "            \n",
    "            list_results.append(risk_vector_gmcm)\n",
    "            cond_params = {\"means\":mus_new,\"stds\":np.sqrt(marginal_covs),\"weights\":alphas.numpy(), \"indexes\":indexes_missing}\n",
    "            conditional_parameters.append(cond_params)\n",
    "    \n",
    "    return (list_results,conditional_parameters)\n",
    "\n",
    "# Example usage\n",
    "precomputed_matrices = precompute_matrices(mus_new=mus_new, covs_new=covs_new, dimension_joint = ndims, n_comps=n_comps)\n",
    "list_results,conditional_parameters = compute_conditional_samples(precomputed_matrices=precomputed_matrices, data_tst_latent=data_tst_latent, \n",
    "                                                                  test_data_with_missing=test_data_with_missing, mus_new=mus_new,\n",
    "                                                                  alphas=alphas, n_comps=n_comps,n_samples = 1000, size_test= np.shape(test_data)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ff3a79-b281-4f4b-b898-eb1478dfa44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "base_dir = \"C:\\\\Users\\\\antoi\\\\Documents\\\\UNIBE\\\\Conditioning\\\\Paper conditioning\\\\experiments\\\\imputation\\\\conditional_parameters\"\n",
    "# Example list of dictionaries\n",
    "\n",
    "\n",
    "# Ensure base directory exists\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over the list of dictionaries\n",
    "for index, params in enumerate(conditional_parameters):\n",
    "    # Create a subfolder for each element\n",
    "    subfolder = os.path.join(base_dir, f'element_{index}')\n",
    "    os.makedirs(subfolder, exist_ok=True)\n",
    "    np.savetxt(os.path.join(subfolder, 'indexes.csv'), params['indexes'], delimiter=',')\n",
    "    # Save each array as a CSV file\n",
    "    np.savetxt(os.path.join(subfolder, 'means.csv'), params['means'], delimiter=',')\n",
    "    np.savetxt(os.path.join(subfolder, 'stds.csv'), params['stds'], delimiter=',')\n",
    "    np.savetxt(os.path.join(subfolder, 'weights.csv'), params['weights'], delimiter=',')\n",
    "\n",
    "print(\"CSV files created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52fdbf35-034f-4441-99ca-eabeeca329c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.3797779083252\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir('C:\\\\Users\\\\antoi\\\\Documents\\\\UNIBE\\\\Conditioning\\\\Paper conditioning\\\\experiments\\\\imputation\\\\before transfo')\n",
    "files = glob.glob('*.csv')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "for i, arr in enumerate(list_results):\n",
    "    df = pd.DataFrame(arr)\n",
    "    df.to_csv(f'array_{i}.csv', index=False)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48382b-6a08-493e-9448-96238418ea2a",
   "metadata": {},
   "source": [
    "Time for doing the inversion with quantiles in R is about 3.6 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67051837-24f0-46d1-91f3-6a55577348a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.106997966766357\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "size_test = np.shape(test_data_with_missing)[0]\n",
    "def impute_datasets(test_data, folder_path, n_imputations=1000):\n",
    "    # Pre-allocate the list for imputed datasets\n",
    "    imputed_datasets = np.array([test_data.copy().values for _ in range(n_imputations)])\n",
    "\n",
    "    # Iterate over each row in the test data\n",
    "    for i, row in test_data.iterrows():\n",
    "        # Load the matrix corresponding to the current row\n",
    "        matrix_file = os.path.join(folder_path, f'matrix{i+1}.csv')\n",
    "        matrix = pd.read_csv(matrix_file, header=0).values\n",
    "\n",
    "        # Get the indices of missing values in the current row\n",
    "        missing_indices = np.where(row.isna())[0]\n",
    "        \n",
    "        # Assign imputed values from the matrix directly\n",
    "        imputed_datasets[:, i, missing_indices] = matrix[:, :len(missing_indices)]\n",
    "\n",
    "    # Convert numpy arrays back to DataFrames\n",
    "    imputed_datasets = [pd.DataFrame(dataset, columns=test_data.columns) for dataset in imputed_datasets]\n",
    "\n",
    "    return imputed_datasets\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "folder_path = 'C:\\\\Users\\\\antoi\\\\Documents\\\\UNIBE\\\\Conditioning\\\\Paper conditioning\\\\experiments\\\\imputation\\\\after transfo quantiles'\n",
    "imputed_datasets_quantiles = impute_datasets(pd.DataFrame(test_data_with_missing), folder_path)\n",
    "\n",
    "organized_data_quantiles = [[] for _ in range(size_test)]\n",
    "\n",
    "# Populate the organized_data list\n",
    "for dataset in imputed_datasets_quantiles:\n",
    "    for i in range(size_test):\n",
    "        organized_data_quantiles[i].append(dataset.iloc[i,:])\n",
    "\n",
    "# Convert each entry in organized_data to a NumPy array (n_imputations, n_dims)\n",
    "for i in range(size_test):\n",
    "    organized_data_quantiles[i] = np.array(organized_data_quantiles[i])\n",
    "end = time.time()\n",
    "total_time_formatting = end-start\n",
    "print(total_time_formatting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e307f8a0-c055-440e-8ea2-b9b1145eb4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.79723525047302\n"
     ]
    }
   ],
   "source": [
    "start_mice = time.time()\n",
    "imputed_datasets = []\n",
    "for i in range(1000):\n",
    "    imp = IterativeImputer(max_iter=10, random_state=i,sample_posterior=True)\n",
    "    imp.fit(df_missing)\n",
    "    data = imp.transform(df_missing)\n",
    "    imputed_datasets.append(data[test_idx,])\n",
    "arrays = np.stack(imputed_datasets)  \n",
    "\n",
    "n_rows = arrays.shape[1]\n",
    "\n",
    "# Now, for each j, extract the j-th row across all matrices\n",
    "result_mice = [arrays[:, j, :] for j in range(n_rows)]\n",
    "end_mice = time.time()\n",
    "print(end_mice-start_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6087346c-5ad8-4532-97b8-ef767b3f3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_score_mice = []\n",
    "energy_score_quantiles= []\n",
    "N = len(organized_data_quantiles)\n",
    "for i in range(N):\n",
    "    energy_score_mice.append(sr.energy_score(test_data[i],\n",
    "    result_mice[i]))\n",
    "    energy_score_quantiles.append(sr.energy_score(test_data[i],\n",
    "    organized_data_quantiles[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5423588a-b3b3-4a94-99d1-60def7dca22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2867351"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(energy_score_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08ab102e-060b-4977-9ccc-37e29bcea58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.252016"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(energy_score_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc4bed-a7e3-41b3-914b-a0bf28cac657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1204c745-1bc4-4436-b97b-496967d39022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\antoi\\Downloads\\GMCM-main\\GMCM-main\\myenv2\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\antoi\\Downloads\\GMCM-main\\GMCM-main\\myenv2\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\antoi\\Downloads\\GMCM-main\\GMCM-main\\myenv2\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copulogram as cp\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd=tfp.distributions\n",
    "tfb=tfp.bijectors\n",
    "from scipy import io\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import mixture\n",
    "import joblib as jbl\n",
    "import sys\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from mixture_models import GMCM, GMC\n",
    "import utils as utl\n",
    "import os\n",
    "from scipy.stats import multivariate_normal\n",
    "import rpy2\n",
    "from transport import transport, extract_marginal_gmms, obtain_quantiles_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81536a35-7e21-43f4-8f3c-6ca145ff7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2910251)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47ce70-80b3-4a62-a4ac-93c730f906da",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b7577-c2a5-4ac4-a5ba-3db3f0160d3c",
   "metadata": {},
   "source": [
    "Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4ee243-59fa-4c2f-9895-35e5529f8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\antoi\\\\Documents\\\\UNIBE\\\\Conditioning/Paper conditioning\\\\experiments\\\\\")\n",
    "data = pd.read_csv(\"wine_data.csv\").iloc[:,0:5].astype('float32').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8214fdf7-0cc3-4f7c-9cc2-901df2ab05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamps,ndims = data.shape\n",
    "n_comps = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a26c1-5c4c-4194-8c50-bf91ff12d9ed",
   "metadata": {},
   "source": [
    "Define dimension and number of modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f2272-c662-4c07-a416-1556c0b61b55",
   "metadata": {},
   "source": [
    "Split the dataset and train the GMCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63467b79-6b52-412b-86e8-3c99445d46ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 178, Number of dimensions = 5\n",
      "Learning Marginals\n",
      "Marginals learnt in 7.98 s.\n",
      "@ Iter:0,                         Training error: -14.0,                         Validation error: nan,                         Time Elapsed: 7.1 s\n",
      "@ Iter:1000,                         Training error: -14.6,                         Validation error: nan,                         Time Elapsed: 14.5 s\n",
      "@ Iter:2000,                         Training error: -15.1,                         Validation error: nan,                         Time Elapsed: 21.7 s\n",
      "@ Iter:3000,                         Training error: -14.7,                         Validation error: nan,                         Time Elapsed: 29.0 s\n",
      "@ Iter:4000,                         Training error: -14.1,                         Validation error: nan,                         Time Elapsed: 36.3 s\n",
      "@ Iter:5000,                         Training error: -14.8,                         Validation error: nan,                         Time Elapsed: 43.5 s\n",
      "@ Iter:6000,                         Training error: -14.8,                         Validation error: nan,                         Time Elapsed: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "data_trn,data_vld,data_tst = utl.splitData(data)\n",
    "\n",
    "print(f'Number of samples = {nsamps}, Number of dimensions = {ndims}')\n",
    "\n",
    "gmcm = GMCM(ndims)\n",
    "gmcm.fit_dist_IFM(data_trn,\n",
    "                n_comps=n_comps,\n",
    "                method=\"GMCM\",\n",
    "                data_vld= None,# fixed number of components\n",
    "                batch_size=10,\n",
    "                max_iters=6001,\n",
    "                regularize=True,\n",
    "                init = 'gmm', #(init option: 'gmm' warm starts gmcm learning compared to 'random')\n",
    "                print_interval=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6104a063-af64-4798-be8e-cfca76f9bffe",
   "metadata": {},
   "source": [
    "Extract parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac774e3-c521-41c6-b930-c175b5e2bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_indexes =  range(ndims)   ## depends on the problem\n",
    "logits,mus,covs,_ = utl.vec2gmm_params(gmcm.ndims,gmcm.ncomps,gmcm.gmc.params)\n",
    "alphas = tf.math.softmax(logits)\n",
    "#dim_remove = list(set(list(range(self.ndims)))-set(dim_list))\n",
    "mus_new = tf.gather(mus,list_indexes, axis=1)\n",
    "covs_new = tf.TensorArray(tf.float32,gmcm.ncomps)\n",
    "for k in range(gmcm.ncomps):\n",
    "    temp_mat = covs[k].numpy()\n",
    "    covs_new = covs_new.write(k,temp_mat[np.ix_(list_indexes, list_indexes)])\n",
    "covs_new = covs_new.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07170a-9546-4c0c-a7e4-9eb9cc9c2645",
   "metadata": {},
   "source": [
    "Transform to latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e51f8d75-ca9d-4bd1-917b-a5d059067304",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tst_uniform = gmcm.marg_bijector.inverse(data_tst)\n",
    "data_tst_latent = gmcm.gmc.distribution.bijector.inverse(data_tst_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb46672-ec61-4c16-b322-c3ef5cc043e2",
   "metadata": {},
   "source": [
    "Define dimension to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c4748a-d98a-44ab-a371-30a914c540e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_predict = 3\n",
    "dimension_joint = ndims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd0a2ab-4b26-46f1-9f3e-a500d971dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\antoi\\\\Downloads\\\\GMCM-main\\\\GMCM-main')\n",
    "from transport import transport, extract_marginal_gmms, obtain_quantiles_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e866aa3-01c0-4c6a-a565-779abca46538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain parameters for marginals and for latents\n",
    "quantiles_weights_list = obtain_quantiles_weights(gmcm)\n",
    "marg_info_list = extract_marginal_gmms(gmcm) # this works fine\n",
    "logits, mus, covs,_ = utl.vec2gmm_params(gmcm.ndims,gmcm.ncomps,gmcm.gmc.params)\n",
    "latent_weights = tf.math.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87ba4eab-4233-43cc-aa30-6eee0a965e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size_test = np.shape(data_tst)[0] # size of the test dataset\n",
    "n_samples = 1000 # number of samples from the conditional distribution that we want to get\n",
    "cond_samples_gmcm = []\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Obtain parameters for marginals and for latents\n",
    "quantiles_weights_list = obtain_quantiles_weights(gmcm)\n",
    "marg_info_list = extract_marginal_gmms(gmcm) # this works fine\n",
    "logits, mus, covs,_ = utl.vec2gmm_params(gmcm.ndims,gmcm.ncomps,gmcm.gmc.params)\n",
    "latent_weights = tf.math.softmax(logits)\n",
    "\n",
    "# Let's precompute the matrices required\n",
    "Sigma22_inv_list = []\n",
    "Sigma12_Sigma22_inv_list = []\n",
    "cov_cond_list = []\n",
    "for j in range(n_comps):\n",
    "    cov_matrix = covs_new[j].numpy()\n",
    "    Sigma22 = cov_matrix[dimension_predict:dimension_joint, dimension_predict:dimension_joint]\n",
    "    Sigma12 = cov_matrix[:dimension_predict, dimension_predict:dimension_joint]\n",
    "    Sigma11 = cov_matrix[:dimension_predict, :dimension_predict]\n",
    "    Sigma22_inv = np.linalg.inv(Sigma22)\n",
    "    Sigma12_Sigma22_inv = Sigma12 @ Sigma22_inv\n",
    "    cov_cond = Sigma11 - Sigma12_Sigma22_inv @ Sigma12.T\n",
    "    Sigma12_Sigma22_inv_list.append(Sigma12_Sigma22_inv)\n",
    "    cov_cond_list.append(cov_cond)\n",
    "parameters = []\n",
    "for i in range(0,size_test):\n",
    "    mean_conds = np.zeros(n_comps)\n",
    "    cov_conds = np.zeros(n_comps)\n",
    "    risk_vector_gmcm = np.zeros((n_samples,dimension_joint))\n",
    "    conditioning_value = data_tst_latent[i,dimension_predict:dimension_joint].numpy() # condition on last variables\n",
    "    new_alphas = np.zeros(n_comps)\n",
    "    risk_vector_gmcm = np.zeros((n_samples, dimension_joint))\n",
    "    for j in range(0,n_comps):\n",
    "        # margin for one of the components\n",
    "        marginal = multivariate_normal.pdf(\n",
    "            conditioning_value,\n",
    "            mean = mus_new[j][dimension_predict:dimension_joint], \n",
    "            cov = covs_new[j][dimension_predict:dimension_joint,dimension_predict:dimension_joint])\n",
    "        # what's this do? compute the *total* marginal density, aggregated over all components?\n",
    "        joint = 0\n",
    "        for k in range(0,n_comps):\n",
    "            joint += alphas[k] * multivariate_normal.pdf(\n",
    "                conditioning_value, mean = mus_new[k][dimension_predict:dimension_joint], \n",
    "                cov = covs_new[k][dimension_predict:dimension_joint,dimension_predict:dimension_joint]\n",
    "            )\n",
    "        # What are 'new alphas'? Is this just prior * likelihood? I.e., prob of a cluster given known values?\n",
    "        if joint != 0:\n",
    "            new_alphas[j] = alphas[j] * (marginal / joint)\n",
    "        else:\n",
    "            new_alphas[j] = 0\n",
    "    \n",
    "    # Deal with rounding error; if new_alphas.sum() == 0, there's a problem somewhere :)\n",
    "    new_alphas /= new_alphas.sum()\n",
    "    # For each sample, pick a cluster\n",
    "    component_choices = np.random.choice(n_comps, size = n_samples, p = new_alphas)\n",
    "    risk_vector_gmcm = np.zeros((n_samples, dimension_joint))\n",
    "    for j in range(0, n_comps):\n",
    "        mask = component_choices == j\n",
    "        n_j = np.sum(mask)\n",
    "        if n_j == 0:\n",
    "            continue\n",
    "        mean_vector = mus_new[j].numpy()\n",
    "        Sigma12_Sigma22_inv = Sigma12_Sigma22_inv_list[j]\n",
    "        cov_cond = cov_cond_list[j]\n",
    "        conditioning_diff = conditioning_value - mean_vector[dimension_predict:dimension_joint]\n",
    "        conditioning_diff = conditioning_diff[:, np.newaxis]\n",
    "        mean_cond_batch = mean_vector[:dimension_predict] + (Sigma12_Sigma22_inv @ conditioning_diff).flatten()\n",
    "        # generate Normal data (in latent space) with required mean and sigma\n",
    "        samples = np.random.multivariate_normal(mean = mean_cond_batch, cov = cov_cond, size = n_j)\n",
    "        risk_vector_gmcm[mask, :dimension_predict] = samples\n",
    "        #mean_conds[j] = mean_cond_batch\n",
    "        #cov_conds [j] = cov_cond\n",
    "    risk_vector_gmcm[:, dimension_predict:dimension_joint] = conditioning_value\n",
    "    #print(i)\n",
    "    risk_vector_gmcm = risk_vector_gmcm.astype('float32')\n",
    "    risk_vector_gmcm = gmcm.gmc.distribution.bijector(risk_vector_gmcm) # to copula (uniform) space\n",
    "    risk_vector_gmcm = gmcm.marg_bijector(risk_vector_gmcm) # transform marginals\n",
    "    #risk_vector_gmcm = transport(risk_vector_gmcm, marg_info_list, latent_weights, mus, covs, quantiles_weights_list)\n",
    "    cond_samples_gmcm.append(risk_vector_gmcm)\n",
    "    #parameters.append([new_alphas,mean_conds,cov_conds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010cc475-ec91-4002-bc8f-8e2d14e36082",
   "metadata": {},
   "source": [
    "Competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a7446a1-a9a6-4b9e-9546-c297d490f9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 178, Number of dimensions = 5\n",
      "Learning Marginals\n",
      "Marginals learnt in 7.5 s.\n"
     ]
    }
   ],
   "source": [
    "n_comps=1\n",
    "nsamps,ndims = data.shape\n",
    "print(f'Number of samples = {nsamps}, Number of dimensions = {ndims}')\n",
    "\n",
    "gc= GMCM(ndims)\n",
    "gc.fit_dist_IFM(data_trn,\n",
    "                 n_comps=n_comps,\n",
    "                 method=\"GC\",\n",
    "                 data_vld= None,# fixed number of components\n",
    "                                    batch_size=10,\n",
    "                                    max_iters=6001,\n",
    "                                    regularize=True,\n",
    "                                    init = 'gmm', #(init option: 'gmm' warm starts gmcm learning compared to 'random')\n",
    "                                    print_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f2b5a7a-53bc-43da-a81f-5d2a91d14a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_indexes = list(range(ndims)) ## depends on the problem\n",
    "logits,mus,covs,_ = utl.vec2gmm_params(gc.ndims,gc.ncomps,gc.gmc.params)\n",
    "alphas = tf.math.softmax(logits)\n",
    "#dim_remove = list(set(list(range(self.ndims)))-set(dim_list))\n",
    "mus_new = tf.gather(mus,list_indexes, axis=1)\n",
    "covs_new = tf.TensorArray(tf.float32,gc.ncomps)\n",
    "for k in range(gc.ncomps):\n",
    "    temp_mat = covs[k].numpy()\n",
    "    covs_new = covs_new.write(k,temp_mat[np.ix_(list_indexes, list_indexes)])\n",
    "covs_new = covs_new.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b73c89e-c491-4ceb-8f36-780b7a5a0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "data_trn_uniform = gc.marg_bijector.inverse(data_trn)\n",
    "data_trn_latent = norm.ppf(data_trn_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e50c97bb-e179-43d0-8787-65ef372a3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trn_uniform = gc.marg_bijector.inverse(data_trn)\n",
    "data_trn_latent = norm.ppf(data_trn_uniform)\n",
    "mean_vector = np.mean(data_trn_latent, axis=0)\n",
    "cov_matrix = np.cov(data_trn_latent, rowvar=False)\n",
    "Sigma22 = cov_matrix[dimension_predict:dimension_joint, dimension_predict:dimension_joint]\n",
    "Sigma12 = cov_matrix[:dimension_predict, dimension_predict:dimension_joint]\n",
    "Sigma11 = cov_matrix[:dimension_predict, :dimension_predict]\n",
    "Sigma22_inv = np.linalg.inv(Sigma22)\n",
    "Sigma12_Sigma22_inv = Sigma12 @ Sigma22_inv\n",
    "cov_cond = Sigma11 - Sigma12_Sigma22_inv @ Sigma12.T\n",
    "from scipy.stats import norm, multivariate_normal as mvn\n",
    "method = \"GC\"\n",
    "if method == \"GC\":\n",
    "    data_tst_uniform = gc.marg_bijector.inverse(data_tst)\n",
    "    data_tst_latent = norm.ppf(data_tst_uniform)\n",
    "    #mean_vector = mus_new[0]\n",
    "    #mean_vector = np.mean(data_trn_latent)\n",
    "    #cov_matrix = n\n",
    "    dimension_predict = dimension_predict # dimension of the vector that we want to predict\n",
    "    dimension_joint = dimension_joint # dimension of the joint distribution\n",
    "    size_test = np.shape(data_tst)[0] # size of the test dataset\n",
    "    n_samples = 1000 # number of samples from the conditional distribution that we want to get\n",
    "    cond_samples_gc = []\n",
    "    #cov_matrix = covs_new[0]\n",
    "    for index in range(data_tst_latent.shape[0]):\n",
    "        risk_vector_gmcm = np.zeros((n_samples,dimension_joint))\n",
    "        conditioning_value = data_tst_latent[index,dimension_predict:dimension_joint]\n",
    "        conditioning_diff = conditioning_value - mean_vector[dimension_predict:dimension_joint]\n",
    "        conditioning_diff = conditioning_diff[:, np.newaxis]\n",
    "        mean_cond_batch = mean_vector[:dimension_predict] + (Sigma12_Sigma22_inv @ conditioning_diff).flatten()\n",
    "        # generate Normal data (in latent space) with required mean and sigma\n",
    "        samples = np.random.multivariate_normal(mean = mean_cond_batch, cov = cov_cond, size = n_samples)\n",
    "        risk_vector_gmcm[:, :dimension_predict] = samples\n",
    "        risk_vector_gmcm[:,dimension_predict:dimension_joint] = conditioning_value\n",
    "        risk_vector_gmcm = risk_vector_gmcm.astype('float32')\n",
    "        risk_vector_gmcm = norm.cdf(risk_vector_gmcm).astype(\"float32\")\n",
    "        risk_vector_gmcm = gc.marg_bijector(risk_vector_gmcm)\n",
    "        cond_samples_gc.append(risk_vector_gmcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd8817-1e0a-4d0c-ba7a-03d4b376c5a8",
   "metadata": {},
   "source": [
    "TGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "738c41f9-77f5-47aa-bfcd-ed663be35041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 178, Number of dimensions = 5\n",
      "Learning Marginals\n",
      "Marginals learnt in 7.54 s.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_comps=3\n",
    "nsamps,ndims = data.shape\n",
    "print(f'Number of samples = {nsamps}, Number of dimensions = {ndims}')\n",
    "\n",
    "tgmm = GMCM(ndims)\n",
    "tgmm.fit_dist_IFM(data_trn,\n",
    "                 n_comps=n_comps,\n",
    "                 method=\"TGMM\",\n",
    "                 data_vld= None,# fixed number of components\n",
    "                                    batch_size=10,\n",
    "                                    max_iters=6001,\n",
    "                                    regularize=True,\n",
    "                                    init = 'gmm', #(init option: 'gmm' warm starts gmcm learning compared to 'random')\n",
    "                                    print_interval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b31ec690-a5c1-4b6e-afc3-235f6c608144",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_indexes = list(range(ndims)) ## depends on the problem\n",
    "logits,mus,covs,_ = utl.vec2gmm_params(tgmm.ndims,tgmm.ncomps,tgmm.gmc.params)\n",
    "alphas = tf.math.softmax(logits)\n",
    "#dim_remove = list(set(list(range(self.ndims)))-set(dim_list))\n",
    "mus_new = tf.gather(mus,list_indexes, axis=1)\n",
    "covs_new = tf.TensorArray(tf.float32,tgmm.ncomps)\n",
    "for k in range(tgmm.ncomps):\n",
    "    temp_mat = covs[k].numpy()\n",
    "    covs_new = covs_new.write(k,temp_mat[np.ix_(list_indexes, list_indexes)])\n",
    "covs_new = covs_new.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2913fb0-1dbd-467a-a327-23296cf71b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"TGMM\"\n",
    "Sigma22_inv_list = []\n",
    "Sigma12_Sigma22_inv_list = []\n",
    "cov_cond_list = []\n",
    "for j in range(n_comps):\n",
    "    cov_matrix = covs_new[j].numpy()\n",
    "    Sigma22 = cov_matrix[dimension_predict:dimension_joint, dimension_predict:dimension_joint]\n",
    "    Sigma12 = cov_matrix[:dimension_predict, dimension_predict:dimension_joint]\n",
    "    Sigma11 = cov_matrix[:dimension_predict, :dimension_predict]\n",
    "    Sigma22_inv = np.linalg.inv(Sigma22)\n",
    "    Sigma12_Sigma22_inv = Sigma12 @ Sigma22_inv\n",
    "    cov_cond = Sigma11 - Sigma12_Sigma22_inv @ Sigma12.T\n",
    "    Sigma12_Sigma22_inv_list.append(Sigma12_Sigma22_inv)\n",
    "    cov_cond_list.append(cov_cond)\n",
    "if method == \"TGMM\":\n",
    "    data_tst_uniform = tgmm.marg_bijector.inverse(data_tst)\n",
    "    data_tst_latent = norm.ppf(data_tst_uniform)\n",
    "    dimension_predict = dimension_predict# dimension of the vector that we want to predict\n",
    "    dimension_joint = dimension_joint # dimension of the joint distribution\n",
    "    size_test = np.shape(data_tst)[0] # size of the test dataset\n",
    "    n_samples = 1000 # number of samples from the conditional distribution that we want to get\n",
    "    cond_samples_tgmm= []\n",
    "    for i in range(0,size_test):\n",
    "        risk_vector_gmcm = np.zeros((n_samples,dimension_joint))\n",
    "        conditioning_value = data_tst_latent[i,dimension_predict:dimension_joint]\n",
    "        new_alphas = np.zeros(n_comps)\n",
    "        # get new weights\n",
    "        for j in range(0,n_comps):\n",
    "            marginal = multivariate_normal.pdf(conditioning_value, mean= mus_new[j][dimension_predict:dimension_joint], \n",
    "                                           cov=covs_new[j][dimension_predict:dimension_joint,dimension_predict:dimension_joint])\n",
    "            joint = 0\n",
    "            for k in range(0,n_comps):\n",
    "                joint += alphas[k] * multivariate_normal.pdf(conditioning_value, mean= mus_new[k][dimension_predict:dimension_joint], \n",
    "                                           cov=covs_new[k][dimension_predict:dimension_joint,dimension_predict:dimension_joint])\n",
    "            if joint != 0:\n",
    "                new_alphas[j] = alphas[j] * (marginal / joint)\n",
    "            else:\n",
    "                new_alphas[j] = 0\n",
    "        new_alphas /= new_alphas.sum()\n",
    "        component_choices = np.random.choice(n_comps, size = n_samples, p = new_alphas)\n",
    "        risk_vector_gmcm = np.zeros((n_samples, dimension_joint))\n",
    "        for j in range(0, n_comps):\n",
    "            mask = component_choices == j\n",
    "            n_j = np.sum(mask)\n",
    "            if n_j == 0:\n",
    "                continue\n",
    "            mean_vector = mus_new[j].numpy()\n",
    "            Sigma12_Sigma22_inv = Sigma12_Sigma22_inv_list[j]\n",
    "            cov_cond = cov_cond_list[j]\n",
    "            conditioning_diff = conditioning_value - mean_vector[dimension_predict:dimension_joint]\n",
    "            conditioning_diff = conditioning_diff[:, np.newaxis]\n",
    "            mean_cond_batch = mean_vector[:dimension_predict] + (Sigma12_Sigma22_inv @ conditioning_diff).flatten()\n",
    "            # generate Normal data (in latent space) with required mean and sigma\n",
    "            samples = np.random.multivariate_normal(mean = mean_cond_batch, cov = cov_cond, size = n_j)\n",
    "            risk_vector_gmcm[mask, :dimension_predict] = samples\n",
    "    \n",
    "        risk_vector_gmcm[:, dimension_predict:dimension_joint] = conditioning_value\n",
    "        risk_vector_gmcm = risk_vector_gmcm.astype('float32')\n",
    "        risk_vector_gmcm = norm.cdf(risk_vector_gmcm).astype(\"float32\")\n",
    "        risk_vector_gmcm = tgmm.marg_bijector(risk_vector_gmcm)\n",
    "        cond_samples_tgmm.append(risk_vector_gmcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763551f-3823-41f1-8abb-ac93a9c25fb1",
   "metadata": {},
   "source": [
    "CKDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55df3971-9459-4637-a7a3-fa9a2f20b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "# Split the training data\n",
    "Y_train = np.reshape(data_trn[:, :dimension_predict],(np.shape(data_trn)[0],dimension_predict))  # First three columns (targets)\n",
    "X_train = np.reshape(data_trn[:, dimension_predict:dimension_joint],(np.shape(data_trn)[0],dimension_joint-dimension_predict))  # Last two columns (features)\n",
    "\n",
    "# Combine both Y and X for KDE fitting\n",
    "train_data = data_trn\n",
    "\n",
    "# Split the test data\n",
    "X_test = data_tst[:, dimension_predict:dimension_joint]  # Last two columns in test data\n",
    "\n",
    "#### Step 2: Fit KDE Model\n",
    "\n",
    "\n",
    "# Fit the KDE model on the joint data\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=1.0).fit(train_data)\n",
    "\n",
    "\n",
    "#### Step 3: Generate Samples from the Conditional Distribution\n",
    "\n",
    "def conditional_sample_kde(kde, X_cond, num_samples=10, tol=0.1):\n",
    "    \"\"\"\n",
    "    Generate samples from the conditional distribution P(Y|X) using the fitted KDE.\n",
    "\n",
    "    Parameters:\n",
    "    kde -- fitted KernelDensity model\n",
    "    X_cond -- ndarray of shape (n_samples, n_features)\n",
    "    num_samples -- number of samples to draw for each condition\n",
    "    tol -- tolerance to filter X samples close to X_cond\n",
    "\n",
    "    Returns:\n",
    "    samples -- ndarray of shape (n_samples, num_samples, n_targets)\n",
    "    \"\"\"\n",
    "               \n",
    "    n_targets = Y_train.shape[1]\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    samples = np.zeros((X_cond.shape[0], num_samples, n_targets))\n",
    "\n",
    "    for i, x in enumerate(X_cond):\n",
    "        # Generate samples from KDE\n",
    "        joint_samples = kde.sample(num_samples * 1000)  # Oversample initially\n",
    "        \n",
    "        # Split into Y and X parts\n",
    "        Y_samples = joint_samples[:, :n_targets]\n",
    "        X_samples = joint_samples[:, n_targets:]\n",
    "        \n",
    "        # Filter samples where X_samples are close to the given X_cond\n",
    "        close_samples_idx = np.linalg.norm(X_samples - x, axis=1) < tol\n",
    "        close_Y_samples = Y_samples[close_samples_idx]\n",
    "        \n",
    "        if len(close_Y_samples) < num_samples:\n",
    "            raise ValueError(\"Not enough samples within tolerance. Increase tol or num_samples.\")\n",
    "\n",
    "        # Randomly choose the needed number of samples\n",
    "        chosen_indices = np.random.choice(len(close_Y_samples), num_samples, replace=False)\n",
    "        samples[i, :, :] = close_Y_samples[chosen_indices]\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Generate samples from the conditional distribution\n",
    "num_samples_per_condition = 1000# Number of samples to draw for each test example\n",
    "tol = 5# Tolerance to filter samples; adjust as needed\n",
    "#tol = 0.4 # Tolerance \n",
    "samples = conditional_sample_kde(kde, X_test, num_samples=num_samples_per_condition, tol=tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db02d5e6-9086-4d1f-a5a2-2dff9620ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scoringrules as sr\n",
    "energy_score_gmcm = []\n",
    "vs_score_gmcm = []\n",
    "energy_score_gc = []\n",
    "vs_score_gc = []\n",
    "energy_score_tgmm = []\n",
    "vs_score_tgmm = []\n",
    "energy_score_ckde = []\n",
    "vs_score_ckde = []\n",
    "for i in range(size_test):\n",
    "    energy_score_gmcm.append(sr.energy_score(data_tst[i,0:dimension_predict],\n",
    "    cond_samples_gmcm[i][:,0:dimension_predict]))\n",
    "    vs_score_gmcm.append(sr.variogram_score(data_tst[i,0:dimension_predict],\n",
    "    cond_samples_gmcm[i][:,0:dimension_predict], p=0.5))\n",
    "    energy_score_gc.append(sr.energy_score(data_tst[i,0:dimension_predict],\n",
    "    cond_samples_gc[i][:,0:dimension_predict]))\n",
    "    vs_score_gc.append(sr.variogram_score(data_tst[i,0:dimension_predict],\n",
    "    cond_samples_gc[i][:,0:dimension_predict], p=0.5))\n",
    "    energy_score_tgmm.append(sr.energy_score(data_tst[i,0:dimension_predict],\n",
    "    cond_samples_tgmm[i][:,0:dimension_predict]))\n",
    "    vs_score_tgmm.append(sr.variogram_score(data_tst[i,0:dimension_predict],\n",
    "    cond_samples_tgmm[i][:,0:dimension_predict], p=0.5))\n",
    "    energy_score_ckde.append(sr.energy_score(data_tst[i,0:dimension_predict],\n",
    "    samples[i][:,0:dimension_predict]))\n",
    "    vs_score_ckde.append(sr.variogram_score(data_tst[i,0:dimension_predict],\n",
    "    samples[i][:,0:dimension_predict], p=0.5))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5429d911-47ee-463d-a74a-ade986c2b9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2905345"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(energy_score_tgmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aad74d2-a1e1-40d3-98fd-98e43ee8eeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2630113"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(energy_score_gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3d35b31-dbba-4901-ae7d-006a27912c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2162371"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(energy_score_gmcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eefc71da-d92b-411f-96de-682a8da2e8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.211293113409495"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(energy_score_ckde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2681a4ec-9827-4aef-83fc-6417e51831e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49349886"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(vs_score_tgmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad46af0d-45bd-4224-98df-9b1f42eefc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5049394"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(vs_score_gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14737688-550e-4be3-9728-c13992732a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49349886"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(vs_score_tgmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3366866-175e-437c-9849-16be888592a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4916509"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(vs_score_gmcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "914c1a1d-681b-4e93-9d69-f560b1b9595d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46866880295664437"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(vs_score_ckde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaee9d3-6d2e-4c1b-af69-00122224bbed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
